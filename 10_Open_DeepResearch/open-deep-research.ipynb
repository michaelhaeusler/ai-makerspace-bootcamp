{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Open Deep Research - Supervisor-Researcher Architecture\n",
    "\n",
    "In this notebook, we'll explore the **supervisor-researcher delegation architecture** for conducting deep research with LangGraph.\n",
    "\n",
    "You can visit this repository to see the original application: [Open Deep Research](https://github.com/langchain-ai/open_deep_research)\n",
    "\n",
    "Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We're Building\n",
    "\n",
    "This implementation uses a **hierarchical delegation pattern** where:\n",
    "\n",
    "1. **User Clarification** - Optionally asks clarifying questions to understand the research scope\n",
    "2. **Research Brief Generation** - Transforms user messages into a structured research brief\n",
    "3. **Supervisor** - A lead researcher that analyzes the brief and delegates research tasks\n",
    "4. **Parallel Researchers** - Multiple sub-agents that conduct focused research simultaneously\n",
    "5. **Research Compression** - Each researcher synthesizes their findings\n",
    "6. **Final Report** - All findings are combined into a comprehensive report\n",
    "\n",
    "![Architecture Diagram 1](images/open-deep-research-1.png)\n",
    "\n",
    "\n",
    "![Architecture Diagram 2](images/open-deep-research-2.png)\n",
    "\n",
    "This differs from a section-based approach by allowing dynamic task decomposition based on the research question, rather than predefined sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "You'll need API keys for Anthropic (for the LLM) and Tavily (for web search). We'll configure the system to use Anthropic's Claude Sonnet 4 exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: State Definitions\n",
    "\n",
    "The state structure is hierarchical with three levels:\n",
    "\n",
    "### Agent State (Top Level)\n",
    "Contains the overall conversation messages, research brief, accumulated notes, and final report.\n",
    "\n",
    "### Supervisor State (Middle Level)\n",
    "Manages the research supervisor's messages, research iterations, and coordinating parallel researchers.\n",
    "\n",
    "### Researcher State (Bottom Level)\n",
    "Each individual researcher has their own message history, tool call iterations, and research findings.\n",
    "\n",
    "We also have structured outputs for tool calling:\n",
    "- **ConductResearch** - Tool for supervisor to delegate research to a sub-agent\n",
    "- **ResearchComplete** - Tool to signal research phase is done\n",
    "- **ClarifyWithUser** - Structured output for asking clarifying questions\n",
    "- **ResearchQuestion** - Structured output for the research brief\n",
    "\n",
    "Let's import these from our library: [`open_deep_library/state.py`](open_deep_library/state.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import state definitions from the library\n",
    "from open_deep_library.state import (\n",
    "    # Main workflow states\n",
    "    AgentState,           # Lines 65-72: Top-level agent state with messages, research_brief, notes, final_report\n",
    "    AgentInputState,      # Lines 62-63: Input state is just messages\n",
    "    \n",
    "    # Supervisor states\n",
    "    SupervisorState,      # Lines 74-81: Supervisor manages research delegation and iterations\n",
    "    \n",
    "    # Researcher states\n",
    "    ResearcherState,      # Lines 83-90: Individual researcher with messages and tool iterations\n",
    "    ResearcherOutputState, # Lines 92-96: Output from researcher (compressed research + raw notes)\n",
    "    \n",
    "    # Structured outputs for tool calling\n",
    "    ConductResearch,      # Lines 15-19: Tool for delegating research to sub-agents\n",
    "    ResearchComplete,     # Lines 21-22: Tool to signal research completion\n",
    "    ClarifyWithUser,      # Lines 30-41: Structured output for user clarification\n",
    "    ResearchQuestion,     # Lines 43-48: Structured output for research brief\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question 1:\n",
    "\n",
    " Explain the interrelationships between the three states.  Why don't we just make a single huge state?\n",
    "\n",
    "#### ✅ Answer 1:\n",
    "\n",
    "We have three types of states: the Main Workflow State, the Supervisor State and the Researcher State.\n",
    "\n",
    "**Main Workflow States**\n",
    "- tracks and coordinates the complete end-to-end research process\n",
    "- it holds the final report and the the overall research brief\n",
    "\n",
    "**Supervisor States**\n",
    "- manages the the task delegation to the researchers\n",
    "- tracks the research iterations and the overall progress\n",
    "\n",
    "**Researcher States**\n",
    "- handles the tasks of a specific researcher\n",
    "- tracks tool call iterations and the overall progress\n",
    "- produces compressed research outputs for the supervisor\n",
    "\n",
    "### **Information flow**\n",
    "\n",
    "### Downward Flow*\n",
    "1. **Main Workflow** → **Supervisor**: Provides the overall `research_brief`\n",
    "2. **Supervisor** → **Researchers**: Delegates specific `research_topic` to each researcher\n",
    "3. **Researchers** receive their individual research tasks, run independable in parallel\n",
    "\n",
    "### Upward Flow\n",
    "1. **Researchers** → **Supervisor**: Send `compressed_research` and `raw_notes`\n",
    "2. **Supervisor** → **Main Workflow**: Provides synthesized findings and overall progress\n",
    "3. **Main Workflow** generates the `final_report`\n",
    "\n",
    "###**Why noton huge state?**\n",
    "\n",
    "**Separation of concerns**\n",
    "- each component has a clear focused responsibility\n",
    "- it is easier to maintain, debug and extend the system\n",
    "\n",
    "**Parallel processing**\n",
    "- multiple researchers can work in parallel\n",
    "- earch researcher has its own independed state\n",
    "\n",
    "**Scalability**\n",
    "- it is easier to add new researchers\n",
    "- logic of components can be changed without affecting other parts of the system\n",
    "\n",
    "\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Utility Functions and Tools\n",
    "\n",
    "The system uses several key utilities:\n",
    "\n",
    "### Search Tools\n",
    "- **tavily_search** - Async web search with automatic summarization to stay within token limits\n",
    "- Supports Anthropic native web search and Tavily API\n",
    "\n",
    "### Reflection Tools\n",
    "- **think_tool** - Allows researchers to reflect on their progress and plan next steps (ReAct pattern)\n",
    "\n",
    "### Helper Utilities\n",
    "- **get_all_tools** - Assembles the complete toolkit (search + MCP + reflection)\n",
    "- **get_today_str** - Provides current date context for research\n",
    "- Token limit handling utilities for graceful degradation\n",
    "\n",
    "These are defined in [`open_deep_library/utils.py`](open_deep_library/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions and tools from the library\n",
    "from open_deep_library.utils import (\n",
    "    # Search tool - Lines 43-136: Tavily search with automatic summarization\n",
    "    tavily_search,\n",
    "    \n",
    "    # Reflection tool - Lines 219-244: Strategic thinking tool for ReAct pattern\n",
    "    think_tool,\n",
    "    \n",
    "    # Tool assembly - Lines 569-597: Get all configured tools\n",
    "    get_all_tools,\n",
    "    \n",
    "    # Date utility - Lines 872-879: Get formatted current date\n",
    "    get_today_str,\n",
    "    \n",
    "    # Supporting utilities for error handling\n",
    "    get_api_key_for_model,          # Lines 892-914: Get API keys from config or env\n",
    "    is_token_limit_exceeded,         # Lines 665-701: Detect token limit errors\n",
    "    get_model_token_limit,           # Lines 831-846: Look up model's token limit\n",
    "    remove_up_to_last_ai_message,    # Lines 848-866: Truncate messages for retry\n",
    "    anthropic_websearch_called,      # Lines 607-637: Detect Anthropic native search usage\n",
    "    openai_websearch_called,         # Lines 639-658: Detect OpenAI native search usage\n",
    "    get_notes_from_tool_calls,       # Lines 599-601: Extract notes from tool messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Question 2:  \n",
    "\n",
    "What are the advantages and disadvantages of importing these components instead of including them in the notebook?\n",
    "\n",
    "\n",
    "### ✅ Answer 2:\n",
    "\n",
    "There are several advantages of importing the components instead of including them in the notebook:\n",
    "\n",
    "- the notebook is much clearer (provided that the naming of the components is such that you can recognice what the component is doing)\n",
    "- no code duplication in the notebook\n",
    "- better maintainability (changes only have to be made once)\n",
    "- functions are grouped together logically\n",
    "- the components can easily be used in other notebooks too\n",
    "- they can easily be shared\n",
    "\n",
    "This only disadvantage I can think of is that you have to look into another file if you need to know exactly what is happening. Also there is a bit more overhead (creating the file, importing it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Configuration System\n",
    "\n",
    "The configuration system controls:\n",
    "\n",
    "### Research Behavior\n",
    "- **allow_clarification** - Whether to ask clarifying questions before research\n",
    "- **max_concurrent_research_units** - How many parallel researchers can run (default: 5)\n",
    "- **max_researcher_iterations** - How many times supervisor can delegate research (default: 6)\n",
    "- **max_react_tool_calls** - Tool call limit per researcher (default: 10)\n",
    "\n",
    "### Model Configuration\n",
    "- **research_model** - Model for research and supervision (we'll use Anthropic)\n",
    "- **compression_model** - Model for synthesizing findings\n",
    "- **final_report_model** - Model for writing the final report\n",
    "- **summarization_model** - Model for summarizing web search results\n",
    "\n",
    "### Search Configuration\n",
    "- **search_api** - Which search API to use (ANTHROPIC, TAVILY, or NONE)\n",
    "- **max_content_length** - Character limit before summarization\n",
    "\n",
    "Defined in [`open_deep_library/configuration.py`](open_deep_library/configuration.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration from the library\n",
    "from open_deep_library.configuration import (\n",
    "    Configuration,    # Lines 38-247: Main configuration class with all settings\n",
    "    SearchAPI,        # Lines 11-17: Enum for search API options (ANTHROPIC, TAVILY, NONE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Prompt Templates\n",
    "\n",
    "The system uses carefully engineered prompts for each phase:\n",
    "\n",
    "### Phase 1: Clarification\n",
    "**clarify_with_user_instructions** - Analyzes if the research scope is clear or needs clarification\n",
    "\n",
    "### Phase 2: Research Brief\n",
    "**transform_messages_into_research_topic_prompt** - Converts user messages into a detailed research brief\n",
    "\n",
    "### Phase 3: Supervisor\n",
    "**lead_researcher_prompt** - System prompt for the supervisor that manages delegation strategy\n",
    "\n",
    "### Phase 4: Researcher\n",
    "**research_system_prompt** - System prompt for individual researchers conducting focused research\n",
    "\n",
    "### Phase 5: Compression\n",
    "**compress_research_system_prompt** - Prompt for synthesizing research findings without losing information\n",
    "\n",
    "### Phase 6: Final Report\n",
    "**final_report_generation_prompt** - Comprehensive prompt for writing the final report\n",
    "\n",
    "All prompts are defined in [`open_deep_library/prompts.py`](open_deep_library/prompts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompt templates from the library\n",
    "from open_deep_library.prompts import (\n",
    "    clarify_with_user_instructions,                    # Lines 3-41: Ask clarifying questions\n",
    "    transform_messages_into_research_topic_prompt,     # Lines 44-77: Generate research brief\n",
    "    lead_researcher_prompt,                            # Lines 79-136: Supervisor system prompt\n",
    "    research_system_prompt,                            # Lines 138-183: Researcher system prompt\n",
    "    compress_research_system_prompt,                   # Lines 186-222: Research compression prompt\n",
    "    final_report_generation_prompt,                    # Lines 228-308: Final report generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Node Functions - The Building Blocks\n",
    "\n",
    "Now let's look at the node functions that make up our graph. We'll import them from the library and understand what each does.\n",
    "\n",
    "### The Complete Research Workflow\n",
    "\n",
    "The workflow consists of 8 key nodes organized into 3 subgraphs:\n",
    "\n",
    "1. **Main Graph Nodes:**\n",
    "   - `clarify_with_user` - Entry point that checks if clarification is needed\n",
    "   - `write_research_brief` - Transforms user input into structured research brief\n",
    "   - `final_report_generation` - Synthesizes all research into final report\n",
    "\n",
    "2. **Supervisor Subgraph Nodes:**\n",
    "   - `supervisor` - Lead researcher that plans and delegates\n",
    "   - `supervisor_tools` - Executes supervisor's tool calls (delegation, reflection)\n",
    "\n",
    "3. **Researcher Subgraph Nodes:**\n",
    "   - `researcher` - Individual researcher conducting focused research\n",
    "   - `researcher_tools` - Executes researcher's tool calls (search, reflection)\n",
    "   - `compress_research` - Synthesizes researcher's findings\n",
    "\n",
    "All nodes are defined in [`open_deep_library/deep_researcher.py`](open_deep_library/deep_researcher.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 1: clarify_with_user\n",
    "\n",
    "**Purpose:** Analyzes user messages and asks clarifying questions if the research scope is unclear.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check if clarification is enabled in configuration\n",
    "2. Use structured output to analyze if clarification is needed\n",
    "3. If needed, end with a clarifying question for the user\n",
    "4. If not needed, proceed to research brief with verification message\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 60-115](open_deep_library/deep_researcher.py#L60-L115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the clarify_with_user node\n",
    "from open_deep_library.deep_researcher import clarify_with_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2: write_research_brief\n",
    "\n",
    "**Purpose:** Transforms user messages into a structured research brief for the supervisor.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Use structured output to generate detailed research brief from messages\n",
    "2. Initialize supervisor with system prompt and research brief\n",
    "3. Set up supervisor messages with proper context\n",
    "\n",
    "**Why this matters:** A well-structured research brief helps the supervisor make better delegation decisions.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 118-175](open_deep_library/deep_researcher.py#L118-L175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the write_research_brief node\n",
    "from open_deep_library.deep_researcher import write_research_brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 3: supervisor\n",
    "\n",
    "**Purpose:** Lead research supervisor that plans research strategy and delegates to sub-researchers.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure model with three tools:\n",
    "   - `ConductResearch` - Delegate research to a sub-agent\n",
    "   - `ResearchComplete` - Signal that research is done\n",
    "   - `think_tool` - Strategic reflection before decisions\n",
    "2. Generate response based on current context\n",
    "3. Increment research iteration count\n",
    "4. Proceed to tool execution\n",
    "\n",
    "**Decision Making:** The supervisor uses `think_tool` to reflect before delegating research, ensuring thoughtful decomposition of the research question.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 178-223](open_deep_library/deep_researcher.py#L178-L223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the supervisor node (from supervisor subgraph)\n",
    "from open_deep_library.deep_researcher import supervisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 4: supervisor_tools\n",
    "\n",
    "**Purpose:** Executes the supervisor's tool calls, including strategic thinking and research delegation.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check exit conditions:\n",
    "   - Exceeded maximum iterations\n",
    "   - No tool calls made\n",
    "   - `ResearchComplete` called\n",
    "2. Process `think_tool` calls for strategic reflection\n",
    "3. Execute `ConductResearch` calls in parallel:\n",
    "   - Spawn researcher subgraphs for each delegation\n",
    "   - Limit to `max_concurrent_research_units` (default: 5)\n",
    "   - Gather all results asynchronously\n",
    "4. Aggregate findings and return to supervisor\n",
    "\n",
    "**Parallel Execution:** This is where the magic happens - multiple researchers work simultaneously on different aspects of the research question.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 225-349](open_deep_library/deep_researcher.py#L225-L349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the supervisor_tools node\n",
    "from open_deep_library.deep_researcher import supervisor_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 5: researcher\n",
    "\n",
    "**Purpose:** Individual researcher that conducts focused research on a specific topic.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Load all available tools (search, MCP, reflection)\n",
    "2. Configure model with tools and researcher system prompt\n",
    "3. Generate response with tool calls\n",
    "4. Increment tool call iteration count\n",
    "\n",
    "**ReAct Pattern:** Researchers use `think_tool` to reflect after each search, deciding whether to continue or provide their answer.\n",
    "\n",
    "**Available Tools:**\n",
    "- Search tools (Tavily or Anthropic native search)\n",
    "- `think_tool` for strategic reflection\n",
    "- `ResearchComplete` to signal completion\n",
    "- MCP tools (if configured)\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 365-424](open_deep_library/deep_researcher.py#L365-L424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the researcher node (from researcher subgraph)\n",
    "from open_deep_library.deep_researcher import researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 6: researcher_tools\n",
    "\n",
    "**Purpose:** Executes the researcher's tool calls, including searches and strategic reflection.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check early exit conditions (no tool calls, native search used)\n",
    "2. Execute all tool calls in parallel:\n",
    "   - Search tools fetch and summarize web content\n",
    "   - `think_tool` records strategic reflections\n",
    "   - MCP tools execute external integrations\n",
    "3. Check late exit conditions:\n",
    "   - Exceeded `max_react_tool_calls` (default: 10)\n",
    "   - `ResearchComplete` called\n",
    "4. Continue research loop or proceed to compression\n",
    "\n",
    "**Error Handling:** Safely handles tool execution errors and continues with available results.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 435-509](open_deep_library/deep_researcher.py#L435-L509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the researcher_tools node\n",
    "from open_deep_library.deep_researcher import researcher_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 7: compress_research\n",
    "\n",
    "**Purpose:** Compresses and synthesizes research findings into a concise, structured summary.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure compression model\n",
    "2. Add compression instruction to messages\n",
    "3. Attempt compression with retry logic:\n",
    "   - If token limit exceeded, remove older messages\n",
    "   - Retry up to 3 times\n",
    "4. Extract raw notes from tool and AI messages\n",
    "5. Return compressed research and raw notes\n",
    "\n",
    "**Why Compression?** Researchers may accumulate lots of tool outputs and reflections. Compression ensures:\n",
    "- All important information is preserved\n",
    "- Redundant information is deduplicated\n",
    "- Content stays within token limits for the final report\n",
    "\n",
    "**Token Limit Handling:** Gracefully handles token limit errors by progressively truncating messages.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 511-585](open_deep_library/deep_researcher.py#L511-L585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the compress_research node\n",
    "from open_deep_library.deep_researcher import compress_research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 8: final_report_generation\n",
    "\n",
    "**Purpose:** Generates the final comprehensive research report from all collected findings.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Extract all notes from completed research\n",
    "2. Configure final report model\n",
    "3. Attempt report generation with retry logic:\n",
    "   - If token limit exceeded, truncate findings by 10%\n",
    "   - Retry up to 3 times\n",
    "4. Return final report or error message\n",
    "\n",
    "**Token Limit Strategy:**\n",
    "- First retry: Use model's token limit × 4 as character limit\n",
    "- Subsequent retries: Reduce by 10% each time\n",
    "- Graceful degradation with helpful error messages\n",
    "\n",
    "**Report Quality:** The prompt guides the model to create well-structured reports with:\n",
    "- Proper headings and sections\n",
    "- Inline citations\n",
    "- Comprehensive coverage of all findings\n",
    "- Sources section at the end\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 607-697](open_deep_library/deep_researcher.py#L607-L697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the final_report_generation node\n",
    "from open_deep_library.deep_researcher import final_report_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Graph Construction - Putting It All Together\n",
    "\n",
    "The system is organized into three interconnected graphs:\n",
    "\n",
    "### 1. Researcher Subgraph (Bottom Level)\n",
    "Handles individual focused research on a specific topic:\n",
    "```\n",
    "START → researcher → researcher_tools → compress_research → END\n",
    "               ↑            ↓\n",
    "               └────────────┘ (loops until max iterations or ResearchComplete)\n",
    "```\n",
    "\n",
    "### 2. Supervisor Subgraph (Middle Level)\n",
    "Manages research delegation and coordination:\n",
    "```\n",
    "START → supervisor → supervisor_tools → END\n",
    "            ↑              ↓\n",
    "            └──────────────┘ (loops until max iterations or ResearchComplete)\n",
    "            \n",
    "supervisor_tools spawns multiple researcher_subgraphs in parallel\n",
    "```\n",
    "\n",
    "### 3. Main Deep Researcher Graph (Top Level)\n",
    "Orchestrates the complete research workflow:\n",
    "```\n",
    "START → clarify_with_user → write_research_brief → research_supervisor → final_report_generation → END\n",
    "                 ↓                                       (supervisor_subgraph)\n",
    "               (may end early if clarification needed)\n",
    "```\n",
    "\n",
    "Let's import the compiled graphs from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pre-compiled graphs from the library\n",
    "from open_deep_library.deep_researcher import (\n",
    "    # Bottom level: Individual researcher workflow\n",
    "    researcher_subgraph,    # Lines 588-605: researcher → researcher_tools → compress_research\n",
    "    \n",
    "    # Middle level: Supervisor coordination\n",
    "    supervisor_subgraph,    # Lines 351-363: supervisor → supervisor_tools (spawns researchers)\n",
    "    \n",
    "    # Top level: Complete research workflow\n",
    "    deep_researcher,        # Lines 699-719: Main graph with all phases\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Architecture?\n",
    "\n",
    "### Advantages of Supervisor-Researcher Delegation\n",
    "\n",
    "1. **Dynamic Task Decomposition**\n",
    "   - Unlike section-based approaches with predefined structure, the supervisor can break down research based on the actual question\n",
    "   - Adapts to different types of research (comparisons, lists, deep dives, etc.)\n",
    "\n",
    "2. **Parallel Execution**\n",
    "   - Multiple researchers work simultaneously on different aspects\n",
    "   - Much faster than sequential section processing\n",
    "   - Configurable parallelism (1-20 concurrent researchers)\n",
    "\n",
    "3. **ReAct Pattern for Quality**\n",
    "   - Researchers use `think_tool` to reflect after each search\n",
    "   - Prevents excessive searching and improves search quality\n",
    "   - Natural stopping conditions based on information sufficiency\n",
    "\n",
    "4. **Flexible Tool Integration**\n",
    "   - Easy to add MCP tools for specialized research\n",
    "   - Supports multiple search APIs (Anthropic, Tavily)\n",
    "   - Each researcher can use different tool combinations\n",
    "\n",
    "5. **Graceful Token Limit Handling**\n",
    "   - Compression prevents token overflow\n",
    "   - Progressive truncation in final report generation\n",
    "   - Research can scale to arbitrary depths\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "- **Complexity:** More moving parts than section-based approach\n",
    "- **Cost:** Parallel researchers use more tokens (but faster)\n",
    "- **Unpredictability:** Research structure emerges dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Running the Deep Researcher\n",
    "\n",
    "Now let's see the system in action! We'll use it to analyze a PDF document about how people use AI.\n",
    "\n",
    "### Setup\n",
    "\n",
    "We need to:\n",
    "1. Load the PDF document\n",
    "2. Configure the execution with Anthropic settings\n",
    "3. Run the research workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PDF with 112460 characters\n",
      "First 500 characters:\n",
      "NBER WORKING PAPER SERIES\n",
      "HOW PEOPLE USE CHATGPT\n",
      "Aaron Chatterji\n",
      "Thomas Cunningham\n",
      "David J. Deming\n",
      "Zoe Hitzig\n",
      "Christopher Ong\n",
      "Carl Yan Shan\n",
      "Kevin Wadman\n",
      "Working Paper 34255\n",
      "http://www.nber.org/papers/w34255\n",
      "NATIONAL BUREAU OF ECONOMIC RESEARCH\n",
      "1050 Massachusetts Avenue\n",
      "Cambridge, MA 02138\n",
      "September 2025\n",
      "We acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan \n",
      "Beiermeister,  Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, \n",
      "Harrison Satcher,  Gawe...\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF document\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "\n",
    "def load_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Load and extract text from PDF.\"\"\"\n",
    "    pdf_text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_text += page.extract_text() + \"\\n\\n\"\n",
    "    return pdf_text\n",
    "\n",
    "# Load the PDF about how people use AI\n",
    "pdf_path = \"data/howpeopleuseai.pdf\"\n",
    "pdf_content = load_pdf(pdf_path)\n",
    "\n",
    "print(f\"Loaded PDF with {len(pdf_content)} characters\")\n",
    "print(f\"First 500 characters:\\n{pdf_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Graph ready for execution\n",
      "  (Note: The graph is pre-compiled from the library)\n"
     ]
    }
   ],
   "source": [
    "# Set up the graph with Anthropic configuration\n",
    "from IPython.display import Markdown, display\n",
    "import uuid\n",
    "\n",
    "# Note: deep_researcher is already compiled from the library\n",
    "# For this demo, we'll use it directly without additional checkpointing\n",
    "graph = deep_researcher\n",
    "\n",
    "print(\"✓ Graph ready for execution\")\n",
    "print(\"  (Note: The graph is pre-compiled from the library)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Anthropic\n",
    "\n",
    "We'll configure the system to use:\n",
    "- **Claude Sonnet 4** for all research, supervision, and report generation\n",
    "- **Tavily** for web search (you can also use Anthropic's native search)\n",
    "- **Moderate parallelism** (3 concurrent researchers)\n",
    "- **Clarification enabled** (will ask if research scope is unclear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration ready\n",
      "  - Research Model: Claude Sonnet 4\n",
      "  - Max Concurrent Researchers: 3\n",
      "  - Max Iterations: 4\n",
      "  - Search API: Tavily\n"
     ]
    }
   ],
   "source": [
    "# Configure for Anthropic with moderate settings\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Model configuration - using Claude Sonnet 4 for everything\n",
    "        \"research_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"research_model_max_tokens\": 10000,\n",
    "        \n",
    "        \"compression_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"compression_model_max_tokens\": 8192,\n",
    "        \n",
    "        \"final_report_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"final_report_model_max_tokens\": 10000,\n",
    "        \n",
    "        \"summarization_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"summarization_model_max_tokens\": 8192,\n",
    "        \n",
    "        # Research behavior\n",
    "        \"allow_clarification\": True,\n",
    "        \"max_concurrent_research_units\": 1,  # 1 parallel researchers\n",
    "        \"max_researcher_iterations\": 2,      # Supervisor can delegate up to 2 times\n",
    "        \"max_react_tool_calls\": 3,           # Each researcher can make up to 3 tool calls\n",
    "        \n",
    "        # Search configuration\n",
    "        \"search_api\": \"tavily\",  # Using Tavily for web search\n",
    "        \"max_content_length\": 50000,\n",
    "        \n",
    "        # Thread ID for this conversation\n",
    "        \"thread_id\": str(uuid.uuid4())\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration ready\")\n",
    "print(f\"  - Research Model: Claude Sonnet 4\")\n",
    "print(f\"  - Max Concurrent Researchers: 3\")\n",
    "print(f\"  - Max Iterations: 4\")\n",
    "print(f\"  - Search API: Tavily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Research\n",
    "\n",
    "Now let's run the research! We'll ask the system to analyze the PDF and provide insights about how people use AI.\n",
    "\n",
    "The workflow will:\n",
    "1. **Clarify** - Check if the request is clear (may skip if obvious)\n",
    "2. **Research Brief** - Transform our request into a structured brief\n",
    "3. **Supervisor** - Plan research strategy and delegate to researchers\n",
    "4. **Parallel Research** - Multiple researchers gather information simultaneously\n",
    "5. **Compression** - Each researcher synthesizes their findings\n",
    "6. **Final Report** - All findings combined into comprehensive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting research workflow...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Node: clarify_with_user\n",
      "============================================================\n",
      "\n",
      "I have sufficient information to proceed with your analysis request. Based on the PDF content you've provided, I understand you want me to analyze the NBER working paper \"How People Use ChatGPT\" and provide insights on:\n",
      "\n",
      "1. Main findings about how people are using AI (specifically ChatGPT)\n",
      "2. Most common use cases identified in the study\n",
      "3. Trends and patterns that emerge from the data\n",
      "\n",
      "The document appears to be a comprehensive research paper with data from ChatGPT's launch in November 2022 through July 2025, including message classifications, usage patterns, and demographic analysis. I will now begin analyzing this document to extract the key insights you've requested.\n",
      "\n",
      "============================================================\n",
      "Node: write_research_brief\n",
      "============================================================\n",
      "\n",
      "Research Brief Generated:\n",
      "I need a comprehensive analysis of the NBER working paper \"How People Use ChatGPT\" (Working Paper No. 34255, September 2025) by Aaron Chatterji et al. The analysis should focus on three specific areas: (1) What are the main findings about how people are using AI, specifically identifying key usage statistics, demographic patterns, growth trends from November 2022 through July 2025, and the shift from work-related to non-work-related usage; (2) What are the most common use cases, particularly exa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n",
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n",
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Node: research_supervisor\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Node: final_report_generation\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FINAL REPORT GENERATED\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Comprehensive Analysis of NBER Working Paper \"How People Use ChatGPT\"\n",
       "\n",
       "## Executive Summary\n",
       "\n",
       "The NBER Working Paper No. 34255 \"How People Use ChatGPT\" by Aaron Chatterji et al. represents the most comprehensive analysis to date of how people actually use AI chatbots. Published in September 2025, this groundbreaking study documents ChatGPT's unprecedented growth from launch in November 2022 through July 2025, when it reached approximately 10% of the world's adult population with over 700 million weekly active users [1]. Using privacy-preserving automated classification methods, the researchers analyzed 1.5 million conversations to reveal fundamental insights about AI adoption patterns, demographic shifts, and usage behaviors that challenge conventional assumptions about how AI is integrated into daily life.\n",
       "\n",
       "## Main Findings About AI Usage Patterns\n",
       "\n",
       "### Unprecedented Growth and Adoption Statistics\n",
       "\n",
       "ChatGPT achieved the fastest global technology diffusion in recorded history. From its launch on November 30, 2022, it reached one million users by December 5th and hit 100 million weekly active users by early November 2023—less than one year after release [3]. By July 2025, the platform served over 700 million weekly active users processing more than 2.6 billion messages daily (over 30,000 messages per second) [3][6]. This had grown to more than 750 million weekly active users by September 2025 [3].\n",
       "\n",
       "The growth trajectory shows remarkable acceleration, with weekly active users doubling every 7-8 months since reaching the 100 million milestone [3][6]. Total message volume increased 5.8x in the final year of the study period while the user base grew 3.2x, indicating that existing users are engaging more intensively as they gain experience with the technology [3][6]. To put this in historical perspective, ChatGPT reached 1 billion messages in December 2024—less than two years after launch—while Google search took eight years to reach 1 billion daily searches after its 1999 public release [3].\n",
       "\n",
       "### Demographic Evolution and Patterns\n",
       "\n",
       "The study reveals dramatic demographic shifts that challenge initial adoption patterns. Early adopters were overwhelmingly male, with more than 80% of initial users having typically male first names [3][5]. However, this gender gap closed rapidly, achieving near parity by early 2025 with 52% of active users having typically female first names by July 2025 [3][5][6]. This represents one of the most significant demographic reversals in technology adoption history.\n",
       "\n",
       "Age distribution shows strong skewing toward younger users, with nearly half of all messages coming from users under 26 years old [1][6][8]. Adults aged 18-25 are responsible for almost half of the platform's 2.6 billion daily messages as of July 2025, suggesting this generation is becoming \"truly AI native\" [6][8].\n",
       "\n",
       "Geographic adoption patterns reveal higher growth rates in lower-income countries compared to wealthy nations. Usage increased 3x in countries from the richest decile but grew 5-6x in middle-income countries [3]. This has resulted in similar usage rates across countries with vastly different GDP per capita levels—Brazil, South Korea, and the US now show comparable adoption despite economic differences [3]. The data indicates minimal difference in ChatGPT usage between countries at the 50th versus 90th percentile of GDP per capita [3].\n",
       "\n",
       "### The Fundamental Shift from Work to Non-Work Usage\n",
       "\n",
       "Perhaps the most significant finding is the dramatic shift in usage patterns from work-related to personal applications. Non-work-related messages grew from 53% of total usage in mid-2024 to over 70% by mid-2025 [1][2][4]. This shift represents a fundamental change in how AI chatbots are being integrated into daily life, with personal and home production uses growing faster than professional applications.\n",
       "\n",
       "Work usage remains concentrated among educated users in highly-paid professional occupations, but represents only about 30% of messages on the consumer platform [4][7]. The decrease in work-related message share is primarily driven by changing usage patterns within existing user cohorts rather than compositional changes in new users [2][4]. This finding has profound implications for economic analysis, suggesting that AI's impact on unpaid activities and home production may be as large or larger than its workplace productivity effects.\n",
       "\n",
       "## Most Common Use Cases and Applications\n",
       "\n",
       "### The Three Dominant Categories\n",
       "\n",
       "The automated classification system revealed that nearly 80% of all ChatGPT conversations fall into three primary categories: Practical Guidance, Seeking Information, and Writing [1][2][4][6]. This concentration indicates that despite AI's theoretical versatility, actual usage patterns are remarkably focused on specific high-value applications.\n",
       "\n",
       "**Practical Guidance** accounts for 29% of all messages and remained stable at this level between July 2024 and July 2025 [6][8]. This category encompasses tutoring and teaching, how-to advice across various domains, and creative ideation. Messages in this category tend to be highly personalized, such as \"I'm a 65-year-old who hurt my left hamstring. Give me some stretches to do\" [8]. The stability of this usage category suggests it represents a core value proposition that users consistently find valuable.\n",
       "\n",
       "**Seeking Information** grew significantly from 14% to 24% of messages between July 2024 and July 2025 [8]. This category functions essentially as a replacement for traditional search engines but with the advantage of providing direct answers without requiring users to scroll through multiple websites [8]. The rapid growth in this category indicates users are increasingly treating ChatGPT as their primary information retrieval tool.\n",
       "\n",
       "**Writing** decreased from 36% to 24% of messages over the study period but remains critical, especially for work-related tasks [8]. Contrary to expectations about generating original content, approximately 66% of writing-related requests involve modifying existing text through editing, critiquing, summarizing, or translating rather than creating new content from scratch [6]. This category includes automated production of emails and documents but emphasizes ChatGPT's unique ability to manipulate and refine textual content compared to traditional search tools.\n",
       "\n",
       "### Work vs. Non-Work Context Analysis\n",
       "\n",
       "In work contexts, Writing dominates usage patterns, accounting for 40% of work-related messages in June 2025 [1]. This reflects ChatGPT's particular strength in producing digital outputs that can be directly integrated into professional workflows. Work-related usage is characterized by more task-oriented interactions, with \"Doing\" (requesting specific outputs) increasing to approximately 56% compared to 40% overall [1].\n",
       "\n",
       "The concentration of work usage among educated, highly-paid professionals suggests that AI chatbots are particularly valuable for knowledge work that involves information synthesis, analysis, and communication. However, the cross-occupational analysis reveals that people in different professions—teachers, salespeople, managers, engineers—use ChatGPT in broadly similar ways, primarily for information gathering, analysis, and decision support rather than profession-specific tasks [7].\n",
       "\n",
       "### Specialized Use Cases\n",
       "\n",
       "Computer programming accounts for only 4.2% of requests, significantly lower than many observers expected given early speculation about coding applications [5][6]. Education represents a more substantial use case with 10% of all requests involving tutoring or teaching [6]. Self-expression represents a relatively small share of overall usage [2][4].\n",
       "\n",
       "The mapping of work-related messages to O*NET occupational categories reveals that \"documenting/recording information\" and \"making decisions and solving problems\" emerge as top messaging categories across nearly every occupation [8]. Notably, even for educators, the primary use case isn't \"training and teaching others\" but \"documenting/recording information,\" and for sales occupations, the top task is \"making decisions and solving problems\" rather than \"selling or influencing others\" [8].\n",
       "\n",
       "## Trends and Patterns from the Data\n",
       "\n",
       "### User Intent Framework Evolution\n",
       "\n",
       "The researchers developed a comprehensive taxonomy showing that user messages fall into three primary intent categories: Asking (49%), Doing (40%), and Expressing (11%) [6][7]. \"Asking\" involves seeking information or advice for decision support and represents the dominant use case, having grown relatively faster than other categories [7]. \"Doing\" involves requesting specific outputs or task completion, while \"Expressing\" encompasses sharing views or feelings without seeking information or action.\n",
       "\n",
       "This framework reveals a fundamental shift from automation-focused interactions to advisory-type interactions, indicating that users increasingly view ChatGPT as a consultation tool rather than a direct task executor [9]. The dominance of \"Asking\" suggests that ChatGPT's primary economic value lies in decision support rather than task automation.\n",
       "\n",
       "### Behavioral Changes Within User Cohorts\n",
       "\n",
       "The study provides unique insights into how individual users' behavior evolves over time. Usage patterns remained relatively flat through most of 2024 but increased substantially beginning in late 2024 to early 2025 across all signup cohorts [3]. Users who signed up in Q3 and Q4 2024 were sending nearly twice as many messages per day by 2025 as they did initially [3]. Early adopters from Q1 2023 were sending 40% more messages per day by July 2025 than they did two years earlier [3].\n",
       "\n",
       "This behavioral evolution suggests that ChatGPT has become substantially better and more user-friendly, leading to increased engagement among existing users rather than just new user acquisition driving growth [3]. The pattern indicates that users develop deeper integration of AI tools into their daily routines over time, finding new applications and becoming more comfortable with the technology.\n",
       "\n",
       "### Economic Value Creation Patterns\n",
       "\n",
       "User satisfaction metrics indicate positive interactions outnumber negative ones by approximately 4:1 [1]. The research suggests significant consumer surplus is being created, with users likely willing to pay substantially more than current subscription fees for the value they receive [7]. This economic value may not immediately appear in GDP or productivity statistics but represents substantial welfare benefits to users [7].\n",
       "\n",
       "The study concludes that ChatGPT's strongest economic value proposition is as a decision-support tool, particularly important for knowledge-intensive work [1][2][4]. Rather than replacing human capabilities, the technology appears to be augmenting human decision-making processes across both professional and personal contexts.\n",
       "\n",
       "### Geographic and Socioeconomic Trends\n",
       "\n",
       "The faster adoption in middle-income countries compared to high-income countries suggests that AI tools may have particularly high value in contexts where access to expert advice or information has been more limited [3]. The convergence in usage rates across countries with different economic development levels indicates that AI tools may help democratize access to certain types of knowledge and assistance.\n",
       "\n",
       "The concentration of work usage among educated professionals while non-work usage grows broadly across demographics suggests different value propositions for different user segments. Professional users appear to value productivity enhancement, while general consumers are finding value in personal assistance and entertainment applications.\n",
       "\n",
       "### Implications for Future AI Development\n",
       "\n",
       "The findings have significant implications for understanding the trajectory of AI integration into society. The study establishes that this generation of young adults is becoming truly AI-native, with usage patterns suggesting deep integration into daily decision-making processes [8]. The shift toward advisory rather than automation uses indicates that future AI development should focus on improving consultation and decision-support capabilities.\n",
       "\n",
       "The research methodology itself sets new standards for privacy-preserving analysis of sensitive user data, demonstrating how comprehensive insights can be generated while maintaining strict privacy protections [3]. The Data Clean Room approach enabled analysis of 1.5 million conversations without researchers ever accessing individual user messages or personal information [3][9].\n",
       "\n",
       "The study represents ground truth data on actual LLM usage patterns, moving beyond speculation to provide evidence-based insights about how AI technologies are being integrated into human activities [9]. These findings suggest that the next decade of software development will be significantly influenced by the advisory model of AI interaction rather than pure automation approaches.\n",
       "\n",
       "### Sources\n",
       "\n",
       "[1] How People Really Use ChatGPT: Findings from NBER Research: https://techmaniacs.com/2025/09/15/how-people-really-use-chatgpt-findings-from-nber-research/\n",
       "\n",
       "[2] How People Use ChatGPT - SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5487080\n",
       "\n",
       "[3] How People Use ChatGPT - by David Deming - Forked Lightning: https://forklightning.substack.com/p/how-people-use-chatgpt\n",
       "\n",
       "[4] How People Use ChatGPT | NBER: https://www.nber.org/papers/w34255\n",
       "\n",
       "[5] OpenAI releases research on ChatGPT usage worldwide - LinkedIn: https://www.linkedin.com/posts/aaron-ronnie-chatterji_this-morning-the-openai-economic-research-activity-7373377911476649986-_n_s\n",
       "\n",
       "[6] NBER Study on ChatGPT Usage: Trends and Insights - LinkedIn: https://www.linkedin.com/posts/vincehan_how-people-use-chatgpt-activity-7374451029079371777-QHL9\n",
       "\n",
       "[7] Event Replay: How People Really Use ChatGPT - OpenAI Forum: https://forum.openai.com/public/videos/event-replay-how-people-really-use-chatgpt-the-who-the-what-and-the-how-of-chatgpt-2025-08-27\n",
       "\n",
       "[8] The reality of wholesale cheating with AI - Tech Xplore: https://techxplore.com/news/2025-10-reality-wholesale-ai.html\n",
       "\n",
       "[9] How People Actually Use ChatGPT — What 1.5M Conversations Tell Us About the Next Decade of Software - Medium: https://medium.com/@adnanmasood/how-people-actually-use-chatgpt-what-1-5m-conversations-tell-us-about-the-next-decade-of-software-ea603212b458"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Research workflow completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create our research request with PDF context\n",
    "research_request = f\"\"\"\n",
    "I have a PDF document about how people use AI. Please analyze this document and provide insights about:\n",
    "\n",
    "1. What are the main findings about how people are using AI?\n",
    "2. What are the most common use cases?\n",
    "3. What trends or patterns emerge from the data?\n",
    "\n",
    "Here's the PDF content:\n",
    "\n",
    "{pdf_content[:10000]}  # First 10k chars to stay within limits\n",
    "\n",
    "...[content truncated for context window]\n",
    "\"\"\"\n",
    "\n",
    "# Execute the graph\n",
    "async def run_research():\n",
    "    \"\"\"Run the research workflow and display results.\"\"\"\n",
    "    print(\"Starting research workflow...\\n\")\n",
    "    \n",
    "    async for event in graph.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": research_request}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        # Display each step\n",
    "        for node_name, node_output in event.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Node: {node_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            if node_name == \"clarify_with_user\":\n",
    "                if \"messages\" in node_output:\n",
    "                    last_msg = node_output[\"messages\"][-1]\n",
    "                    print(f\"\\n{last_msg.content}\")\n",
    "            \n",
    "            elif node_name == \"write_research_brief\":\n",
    "                if \"research_brief\" in node_output:\n",
    "                    print(f\"\\nResearch Brief Generated:\")\n",
    "                    print(f\"{node_output['research_brief'][:500]}...\")\n",
    "            \n",
    "            elif node_name == \"supervisor\":\n",
    "                print(f\"\\nSupervisor planning research strategy...\")\n",
    "                if \"supervisor_messages\" in node_output:\n",
    "                    last_msg = node_output[\"supervisor_messages\"][-1]\n",
    "                    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n",
    "                        print(f\"Tool calls: {len(last_msg.tool_calls)}\")\n",
    "                        for tc in last_msg.tool_calls:\n",
    "                            print(f\"  - {tc['name']}\")\n",
    "            \n",
    "            elif node_name == \"supervisor_tools\":\n",
    "                print(f\"\\nExecuting supervisor's tool calls...\")\n",
    "                if \"notes\" in node_output:\n",
    "                    print(f\"Research notes collected: {len(node_output['notes'])}\")\n",
    "            \n",
    "            elif node_name == \"final_report_generation\":\n",
    "                if \"final_report\" in node_output:\n",
    "                    print(f\"\\n\" + \"=\"*60)\n",
    "                    print(\"FINAL REPORT GENERATED\")\n",
    "                    print(\"=\"*60 + \"\\n\")\n",
    "                    display(Markdown(node_output[\"final_report\"]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Research workflow completed!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the research\n",
    "await run_research()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Output\n",
    "\n",
    "Let's break down what happened:\n",
    "\n",
    "### Phase 1: Clarification\n",
    "The system checked if your request was clear. Since you provided a PDF and specific questions, it likely proceeded without clarification.\n",
    "\n",
    "### Phase 2: Research Brief\n",
    "Your request was transformed into a detailed research brief that guides the supervisor's delegation strategy.\n",
    "\n",
    "### Phase 3: Supervisor Delegation\n",
    "The supervisor analyzed the brief and decided how to break down the research:\n",
    "- Used `think_tool` to plan strategy\n",
    "- Called `ConductResearch` multiple times to delegate to parallel researchers\n",
    "- Each delegation specified a focused research topic\n",
    "\n",
    "### Phase 4: Parallel Research\n",
    "Multiple researchers worked simultaneously:\n",
    "- Each researcher used web search tools to gather information\n",
    "- Used `think_tool` to reflect after each search\n",
    "- Decided when they had enough information\n",
    "- Compressed their findings into clean summaries\n",
    "\n",
    "### Phase 5: Final Report\n",
    "All research findings were synthesized into a comprehensive report with:\n",
    "- Well-structured sections\n",
    "- Inline citations\n",
    "- Sources listed at the end\n",
    "- Balanced coverage of all findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🏗️ Activity #1: Try Different Configurations\n",
    "\n",
    "You can experiment with different settings to see how they affect the research.  You may select three or more of the following settings (or invent your own experiments) and describe the results.\n",
    "\n",
    "### Increase Parallelism\n",
    "```python\n",
    "\"max_concurrent_research_units\": 10  # More researchers working simultaneously\n",
    "```\n",
    "\n",
    "### Deeper Research\n",
    "```python\n",
    "\"max_researcher_iterations\": 8   # Supervisor can delegate more times\n",
    "\"max_react_tool_calls\": 15      # Each researcher can search more\n",
    "```\n",
    "\n",
    "### Use Anthropic Native Search\n",
    "```python\n",
    "\"search_api\": \"anthropic\"  # Use Claude's built-in web search\n",
    "```\n",
    "\n",
    "### Disable Clarification\n",
    "```python\n",
    "\"allow_clarification\": False  # Skip clarification phase\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Architecture Benefits\n",
    "1. **Dynamic Decomposition** - Research structure emerges from the question, not predefined\n",
    "2. **Parallel Efficiency** - Multiple researchers work simultaneously\n",
    "3. **ReAct Quality** - Strategic reflection improves search decisions\n",
    "4. **Scalability** - Handles token limits gracefully through compression\n",
    "5. **Flexibility** - Easy to add new tools and capabilities\n",
    "\n",
    "### When to Use This Pattern\n",
    "- **Complex research questions** that need multi-angle investigation\n",
    "- **Comparison tasks** where parallel research on different topics is beneficial\n",
    "- **Open-ended exploration** where structure should emerge dynamically\n",
    "- **Time-sensitive research** where parallel execution speeds up results\n",
    "\n",
    "### When to Use Section-Based Instead\n",
    "- **Highly structured reports** with predefined format requirements\n",
    "- **Template-based content** where sections are always the same\n",
    "- **Sequential dependencies** where later sections depend on earlier ones\n",
    "- **Budget constraints** where token efficiency is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Extend the System\n",
    "1. **Add MCP Tools** - Integrate specialized tools for your domain\n",
    "2. **Custom Prompts** - Modify prompts for specific research types\n",
    "3. **Different Models** - Try different Claude versions or mix models\n",
    "4. **Persistence** - Use a real database for checkpointing instead of memory\n",
    "\n",
    "### Learn More\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [Open Deep Research Repo](https://github.com/langchain-ai/open_deep_research)\n",
    "- [Anthropic Claude Documentation](https://docs.anthropic.com/)\n",
    "- [Tavily Search API](https://tavily.com/)\n",
    "\n",
    "### Deploy\n",
    "- Use LangGraph Cloud for production deployment\n",
    "- Add proper error handling and logging\n",
    "- Implement rate limiting and cost controls\n",
    "- Monitor research quality and costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
